{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Getting Started Welcome Welcome to the timm documentation, a lean set of docs that covers the basics of timm . For a more comprehensive set of docs (currently under development), please visit timmdocs by Aman Arora . Install The library can be installed with pip: pip install timm I update the PyPi (pip) packages when I'm confident there are no significant model regressions from previous releases. If you want to pip install the bleeding edge from GitHub, use: pip install git+https://github.com/rwightman/pytorch-image-models.git Conda Environment All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems, specifically Python 3.6.x, 3.7.x., 3.8.x., 3.9 Little to no care has been taken to be Python 2.x friendly and will not support it. If you run into any challenges running on Windows, or other OS, I'm definitely open to looking into those issues so long as it's in a reproducible (read Conda) environment. PyTorch versions 1.4, 1.5.x, 1.6, 1.7.x, and 1.8 have been tested with this code. I've tried to keep the dependencies minimal, the setup is as per the PyTorch default install instructions for Conda: conda create -n torch-env conda activate torch-env conda install pytorch torchvision cudatoolkit=11.1 -c pytorch -c conda-forge conda install pyyaml Load a Pretrained Model Pretrained models can be loaded using timm.create_model import timm m = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True ) m . eval () List Models with Pretrained Weights import timm from pprint import pprint model_names = timm . list_models ( pretrained = True ) pprint ( model_names ) >>> [ 'adv_inception_v3' , 'cspdarknet53' , 'cspresnext50' , 'densenet121' , 'densenet161' , 'densenet169' , 'densenet201' , 'densenetblur121d' , 'dla34' , 'dla46_c' , ... ] List Model Architectures by Wildcard import timm from pprint import pprint model_names = timm . list_models ( '*resne*t*' ) pprint ( model_names ) >>> [ 'cspresnet50' , 'cspresnet50d' , 'cspresnet50w' , 'cspresnext50' , ... ]","title":"Getting Started"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#welcome","text":"Welcome to the timm documentation, a lean set of docs that covers the basics of timm . For a more comprehensive set of docs (currently under development), please visit timmdocs by Aman Arora .","title":"Welcome"},{"location":"#install","text":"The library can be installed with pip: pip install timm I update the PyPi (pip) packages when I'm confident there are no significant model regressions from previous releases. If you want to pip install the bleeding edge from GitHub, use: pip install git+https://github.com/rwightman/pytorch-image-models.git Conda Environment All development and testing has been done in Conda Python 3 environments on Linux x86-64 systems, specifically Python 3.6.x, 3.7.x., 3.8.x., 3.9 Little to no care has been taken to be Python 2.x friendly and will not support it. If you run into any challenges running on Windows, or other OS, I'm definitely open to looking into those issues so long as it's in a reproducible (read Conda) environment. PyTorch versions 1.4, 1.5.x, 1.6, 1.7.x, and 1.8 have been tested with this code. I've tried to keep the dependencies minimal, the setup is as per the PyTorch default install instructions for Conda: conda create -n torch-env conda activate torch-env conda install pytorch torchvision cudatoolkit=11.1 -c pytorch -c conda-forge conda install pyyaml","title":"Install"},{"location":"#load-a-pretrained-model","text":"Pretrained models can be loaded using timm.create_model import timm m = timm . create_model ( 'mobilenetv3_large_100' , pretrained = True ) m . eval ()","title":"Load a Pretrained Model"},{"location":"#list-models-with-pretrained-weights","text":"import timm from pprint import pprint model_names = timm . list_models ( pretrained = True ) pprint ( model_names ) >>> [ 'adv_inception_v3' , 'cspdarknet53' , 'cspresnext50' , 'densenet121' , 'densenet161' , 'densenet169' , 'densenet201' , 'densenetblur121d' , 'dla34' , 'dla46_c' , ... ]","title":"List Models with Pretrained Weights"},{"location":"#list-model-architectures-by-wildcard","text":"import timm from pprint import pprint model_names = timm . list_models ( '*resne*t*' ) pprint ( model_names ) >>> [ 'cspresnet50' , 'cspresnet50d' , 'cspresnet50w' , 'cspresnext50' , ... ]","title":"List Model Architectures by Wildcard"},{"location":"archived_changes/","text":"Archived Changes Aug 12, 2020 New/updated weights from training experiments EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp) RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver) CSPResNet50 - 79.6 top-1 (76.6 from official ver) Add CutMix integrated w/ Mixup. See pull request for some usage examples Some fixes for using pretrained weights with in_chans != 3 on several models. Aug 5, 2020 Universal feature extraction, new models, new weights, new test sets. * All models support the features_only=True argument for create_model call to return a network that extracts feature maps from the deepest layer at each stride. * New models * CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet * ReXNet * (Modified Aligned) Xception41/65/71 (a proper port of TF models) * New trained weights * SEResNet50 - 80.3 top-1 * CSPDarkNet53 - 80.1 top-1 * CSPResNeXt50 - 80.0 top-1 * DPN68b - 79.2 top-1 * EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by @hal-314 ) * Add 'real' labels for ImageNet and ImageNet-Renditions test set, see results/README.md * Test set ranking/top-n diff script by @KushajveerSingh * Train script and loader/transform tweaks to punch through more aug arguments * README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/ * adamp and sgdp optimizers added by @hellbell June 11, 2020 Bunch of changes: * DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions * VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1 * Activation factory added along with new activations: * select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export) * hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish * context mgr for setting exportable/scriptable/no_jit states * Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface * Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this * Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call * Prep for 0.1.28 pip release May 12, 2020 Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt , paper https://arxiv.org/abs/2004.08955 )) May 3, 2020 Pruned EfficientNet B1, B2, and B3 ( https://arxiv.org/abs/2002.08258 ) contributed by Yonathan Aflalo May 1, 2020 Merged a number of execellent contributions in the ResNet model family over the past month BlurPool2D and resnetblur models initiated by Chris Ha , I trained resnetblur50 to 79.3. TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by mrT23 ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per ( https://arxiv.org/abs/2002.08258 ) by Yonathan Aflalo 200 pretrained models in total now with updated results csv in results folder April 5, 2020 Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3% March 18, 2020 Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams) April 5, 2020 Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3% March 18, 2020 Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams) Feb 29, 2020 New MobileNet-V3 Large weights trained from stratch with this code to 75.77% top-1 IMPORTANT CHANGE - default weight init changed for all MobilenetV3 / EfficientNet / related models overall results similar to a bit better training from scratch on a few smaller models tried performance early in training seems consistently improved but less difference by end set fix_group_fanout=False in _init_weight_goog fn if you need to reproducte past behaviour Experimental LR noise feature added applies a random perturbation to LR each epoch in specified range of training Feb 18, 2020 Big refactor of model layers and addition of several attention mechanisms. Several additions motivated by 'Compounding the Performance Improvements...' ( https://arxiv.org/abs/2001.06268 ): Move layer/module impl into layers subfolder/module of models and organize in a more granular fashion ResNet downsample paths now properly support dilation (output stride != 32) for avg_pool ('D' variant) and 3x3 (SENets) networks Add Selective Kernel Nets on top of ResNet base, pretrained weights skresnet18 - 73% top-1 skresnet34 - 76.9% top-1 skresnext50_32x4d (equiv to SKNet50) - 80.2% top-1 ECA and CECA (circular padding) attention layer contributed by Chris Ha CBAM attention experiment (not the best results so far, may remove) Attention factory to allow dynamically selecting one of SE, ECA, CBAM in the .se position for all ResNets Add DropBlock and DropPath (formerly DropConnect for EfficientNet/MobileNetv3) support to all ResNet variants Full dataset results updated that incl NoisyStudent weights and 2 of the 3 SK weights Feb 12, 2020 Add EfficientNet-L2 and B0-B7 NoisyStudent weights ported from Tensorflow TPU Feb 6, 2020 Add RandAugment trained EfficientNet-ES (EdgeTPU-Small) weights with 78.1 top-1. Trained by Andrew Lavin (see Training section for hparams) Feb \u00bd, 2020 Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 AdvProp, different input normalization. Update results csv files on all models for ImageNet validation and three other test sets Push PyPi package update Jan 31, 2020 Update ResNet50 weights with a new 79.038 result from further JSD / AugMix experiments. Full command line for reproduction in training section below. Jan 11/12, 2020 Master may be a bit unstable wrt to training, these changes have been tested but not all combos Implementations of AugMix added to existing RA and AA. Including numerous supporting pieces like JSD loss (Jensen-Shannon divergence + CE), and AugMixDataset SplitBatchNorm adaptation layer added for implementing Auxiliary BN as per AdvProp paper ResNet-50 AugMix trained model w/ 79% top-1 added seresnext26tn_32x4d - 77.99 top-1, 93.75 top-5 added to tiered experiment, higher img/s than 't' and 'd' Jan 3, 2020 Add RandAugment trained EfficientNet-B0 weight with 77.7 top-1. Trained by Michael Klachko with this code and recent hparams (see Training section) Add avg_checkpoints.py script for post training weight averaging and update all scripts with header docstrings and shebangs. Dec 30, 2019 Merge Dushyant Mehta's PR for SelecSLS (Selective Short and Long Range Skip Connections) networks. Good GPU memory consumption and throughput. Original: https://github.com/mehtadushy/SelecSLS-Pytorch Dec 28, 2019 Add new model weights and training hparams (see Training Hparams section) efficientnet_b3 - 81.5 top-1, 95.7 top-5 at default res/crop, 81.9, 95.8 at 320x320 1.0 crop-pct trained with RandAugment, ended up with an interesting but less than perfect result (see training section) seresnext26d_32x4d - 77.6 top-1, 93.6 top-5 deep stem (32, 32, 64), avgpool downsample stem/dowsample from bag-of-tricks paper seresnext26t_32x4d - 78.0 top-1, 93.7 top-5 deep tiered stem (24, 48, 64), avgpool downsample (a modified 'D' variant) stem sizing mods from Jeremy Howard and fastai devs discussing ResNet architecture experiments Dec 23, 2019 Add RandAugment trained MixNet-XL weights with 80.48 top-1. --dist-bn argument added to train.py, will distribute BN stats between nodes after each train epoch, before eval Dec 4, 2019 Added weights from the first training from scratch of an EfficientNet (B2) with my new RandAugment implementation. Much better than my previous B2 and very close to the official AdvProp ones (80.4 top-1, 95.08 top-5). Nov 29, 2019 Brought EfficientNet and MobileNetV3 up to date with my https://github.com/rwightman/gen-efficientnet-pytorch code. Torchscript and ONNX export compat excluded. AdvProp weights added Official TF MobileNetv3 weights added EfficientNet and MobileNetV3 hook based 'feature extraction' classes added. Will serve as basis for using models as backbones in obj detection/segmentation tasks. Lots more to be done here... HRNet classification models and weights added from https://github.com/HRNet/HRNet-Image-Classification Consistency in global pooling, reset_classifer , and forward_features across models forward_features always returns unpooled feature maps now Reasonable chance I broke something... let me know Nov 22, 2019 Add ImageNet training RandAugment implementation alongside AutoAugment. PyTorch Transform compatible format, using PIL. Currently training two EfficientNet models from scratch with promising results... will update. drop-connect cmd line arg finally added to train.py , no need to hack model fns. Works for efficientnet/mobilenetv3 based models, ignored otherwise.","title":"Archived Changes"},{"location":"archived_changes/#archived-changes","text":"","title":"Archived Changes"},{"location":"archived_changes/#aug-12-2020","text":"New/updated weights from training experiments EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp) RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver) CSPResNet50 - 79.6 top-1 (76.6 from official ver) Add CutMix integrated w/ Mixup. See pull request for some usage examples Some fixes for using pretrained weights with in_chans != 3 on several models.","title":"Aug 12, 2020"},{"location":"archived_changes/#aug-5-2020","text":"Universal feature extraction, new models, new weights, new test sets. * All models support the features_only=True argument for create_model call to return a network that extracts feature maps from the deepest layer at each stride. * New models * CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet * ReXNet * (Modified Aligned) Xception41/65/71 (a proper port of TF models) * New trained weights * SEResNet50 - 80.3 top-1 * CSPDarkNet53 - 80.1 top-1 * CSPResNeXt50 - 80.0 top-1 * DPN68b - 79.2 top-1 * EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by @hal-314 ) * Add 'real' labels for ImageNet and ImageNet-Renditions test set, see results/README.md * Test set ranking/top-n diff script by @KushajveerSingh * Train script and loader/transform tweaks to punch through more aug arguments * README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/ * adamp and sgdp optimizers added by @hellbell","title":"Aug 5, 2020"},{"location":"archived_changes/#june-11-2020","text":"Bunch of changes: * DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions * VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1 * Activation factory added along with new activations: * select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export) * hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish * context mgr for setting exportable/scriptable/no_jit states * Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface * Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this * Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call * Prep for 0.1.28 pip release","title":"June 11, 2020"},{"location":"archived_changes/#may-12-2020","text":"Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt , paper https://arxiv.org/abs/2004.08955 ))","title":"May 12, 2020"},{"location":"archived_changes/#may-3-2020","text":"Pruned EfficientNet B1, B2, and B3 ( https://arxiv.org/abs/2002.08258 ) contributed by Yonathan Aflalo","title":"May 3, 2020"},{"location":"archived_changes/#may-1-2020","text":"Merged a number of execellent contributions in the ResNet model family over the past month BlurPool2D and resnetblur models initiated by Chris Ha , I trained resnetblur50 to 79.3. TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by mrT23 ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per ( https://arxiv.org/abs/2002.08258 ) by Yonathan Aflalo 200 pretrained models in total now with updated results csv in results folder","title":"May 1, 2020"},{"location":"archived_changes/#april-5-2020","text":"Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3%","title":"April 5, 2020"},{"location":"archived_changes/#march-18-2020","text":"Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams)","title":"March 18, 2020"},{"location":"archived_changes/#april-5-2020_1","text":"Add some newly trained MobileNet-V2 models trained with latest h-params, rand augment. They compare quite favourably to EfficientNet-Lite 3.5M param MobileNet-V2 100 @ 73% 4.5M param MobileNet-V2 110d @ 75% 6.1M param MobileNet-V2 140 @ 76.5% 5.8M param MobileNet-V2 120d @ 77.3%","title":"April 5, 2020"},{"location":"archived_changes/#march-18-2020_1","text":"Add EfficientNet-Lite models w/ weights ported from Tensorflow TPU Add RandAugment trained ResNeXt-50 32x4d weights with 79.8 top-1. Trained by Andrew Lavin (see Training section for hparams)","title":"March 18, 2020"},{"location":"archived_changes/#feb-29-2020","text":"New MobileNet-V3 Large weights trained from stratch with this code to 75.77% top-1 IMPORTANT CHANGE - default weight init changed for all MobilenetV3 / EfficientNet / related models overall results similar to a bit better training from scratch on a few smaller models tried performance early in training seems consistently improved but less difference by end set fix_group_fanout=False in _init_weight_goog fn if you need to reproducte past behaviour Experimental LR noise feature added applies a random perturbation to LR each epoch in specified range of training","title":"Feb 29, 2020"},{"location":"archived_changes/#feb-18-2020","text":"Big refactor of model layers and addition of several attention mechanisms. Several additions motivated by 'Compounding the Performance Improvements...' ( https://arxiv.org/abs/2001.06268 ): Move layer/module impl into layers subfolder/module of models and organize in a more granular fashion ResNet downsample paths now properly support dilation (output stride != 32) for avg_pool ('D' variant) and 3x3 (SENets) networks Add Selective Kernel Nets on top of ResNet base, pretrained weights skresnet18 - 73% top-1 skresnet34 - 76.9% top-1 skresnext50_32x4d (equiv to SKNet50) - 80.2% top-1 ECA and CECA (circular padding) attention layer contributed by Chris Ha CBAM attention experiment (not the best results so far, may remove) Attention factory to allow dynamically selecting one of SE, ECA, CBAM in the .se position for all ResNets Add DropBlock and DropPath (formerly DropConnect for EfficientNet/MobileNetv3) support to all ResNet variants Full dataset results updated that incl NoisyStudent weights and 2 of the 3 SK weights","title":"Feb 18, 2020"},{"location":"archived_changes/#feb-12-2020","text":"Add EfficientNet-L2 and B0-B7 NoisyStudent weights ported from Tensorflow TPU","title":"Feb 12, 2020"},{"location":"archived_changes/#feb-6-2020","text":"Add RandAugment trained EfficientNet-ES (EdgeTPU-Small) weights with 78.1 top-1. Trained by Andrew Lavin (see Training section for hparams)","title":"Feb 6, 2020"},{"location":"archived_changes/#feb-12-2020_1","text":"Port new EfficientNet-B8 (RandAugment) weights, these are different than the B8 AdvProp, different input normalization. Update results csv files on all models for ImageNet validation and three other test sets Push PyPi package update","title":"Feb 1/2, 2020"},{"location":"archived_changes/#jan-31-2020","text":"Update ResNet50 weights with a new 79.038 result from further JSD / AugMix experiments. Full command line for reproduction in training section below.","title":"Jan 31, 2020"},{"location":"archived_changes/#jan-1112-2020","text":"Master may be a bit unstable wrt to training, these changes have been tested but not all combos Implementations of AugMix added to existing RA and AA. Including numerous supporting pieces like JSD loss (Jensen-Shannon divergence + CE), and AugMixDataset SplitBatchNorm adaptation layer added for implementing Auxiliary BN as per AdvProp paper ResNet-50 AugMix trained model w/ 79% top-1 added seresnext26tn_32x4d - 77.99 top-1, 93.75 top-5 added to tiered experiment, higher img/s than 't' and 'd'","title":"Jan 11/12, 2020"},{"location":"archived_changes/#jan-3-2020","text":"Add RandAugment trained EfficientNet-B0 weight with 77.7 top-1. Trained by Michael Klachko with this code and recent hparams (see Training section) Add avg_checkpoints.py script for post training weight averaging and update all scripts with header docstrings and shebangs.","title":"Jan 3, 2020"},{"location":"archived_changes/#dec-30-2019","text":"Merge Dushyant Mehta's PR for SelecSLS (Selective Short and Long Range Skip Connections) networks. Good GPU memory consumption and throughput. Original: https://github.com/mehtadushy/SelecSLS-Pytorch","title":"Dec 30, 2019"},{"location":"archived_changes/#dec-28-2019","text":"Add new model weights and training hparams (see Training Hparams section) efficientnet_b3 - 81.5 top-1, 95.7 top-5 at default res/crop, 81.9, 95.8 at 320x320 1.0 crop-pct trained with RandAugment, ended up with an interesting but less than perfect result (see training section) seresnext26d_32x4d - 77.6 top-1, 93.6 top-5 deep stem (32, 32, 64), avgpool downsample stem/dowsample from bag-of-tricks paper seresnext26t_32x4d - 78.0 top-1, 93.7 top-5 deep tiered stem (24, 48, 64), avgpool downsample (a modified 'D' variant) stem sizing mods from Jeremy Howard and fastai devs discussing ResNet architecture experiments","title":"Dec 28, 2019"},{"location":"archived_changes/#dec-23-2019","text":"Add RandAugment trained MixNet-XL weights with 80.48 top-1. --dist-bn argument added to train.py, will distribute BN stats between nodes after each train epoch, before eval","title":"Dec 23, 2019"},{"location":"archived_changes/#dec-4-2019","text":"Added weights from the first training from scratch of an EfficientNet (B2) with my new RandAugment implementation. Much better than my previous B2 and very close to the official AdvProp ones (80.4 top-1, 95.08 top-5).","title":"Dec 4, 2019"},{"location":"archived_changes/#nov-29-2019","text":"Brought EfficientNet and MobileNetV3 up to date with my https://github.com/rwightman/gen-efficientnet-pytorch code. Torchscript and ONNX export compat excluded. AdvProp weights added Official TF MobileNetv3 weights added EfficientNet and MobileNetV3 hook based 'feature extraction' classes added. Will serve as basis for using models as backbones in obj detection/segmentation tasks. Lots more to be done here... HRNet classification models and weights added from https://github.com/HRNet/HRNet-Image-Classification Consistency in global pooling, reset_classifer , and forward_features across models forward_features always returns unpooled feature maps now Reasonable chance I broke something... let me know","title":"Nov 29, 2019"},{"location":"archived_changes/#nov-22-2019","text":"Add ImageNet training RandAugment implementation alongside AutoAugment. PyTorch Transform compatible format, using PIL. Currently training two EfficientNet models from scratch with promising results... will update. drop-connect cmd line arg finally added to train.py , no need to hack model fns. Works for efficientnet/mobilenetv3 based models, ignored otherwise.","title":"Nov 22, 2019"},{"location":"changes/","text":"Recent Changes March 7, 2021 First 0.4.x PyPi release w/ NFNets (& related), ByoB (GPU-Efficient, RepVGG, etc). Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation. Feb 18, 2021 Add pretrained weights and model variants for NFNet-F* models from DeepMind Haiku impl . Models are prefixed with dm_ . They require SAME padding conv, skipinit enabled, and activation gains applied in act fn. These models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly \u00bd the inference speed of my SiLU PyTorch optimized s variants. Original model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated). Matching the original pre-processing as closely as possible I get these results: dm_nfnet_f6 - 86.352 dm_nfnet_f5 - 86.100 dm_nfnet_f4 - 85.834 dm_nfnet_f3 - 85.676 dm_nfnet_f2 - 85.178 dm_nfnet_f1 - 84.696 dm_nfnet_f0 - 83.464 Feb 16, 2021 Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171 . Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py. AGC w/ default clipping factor --clip-grad .01 --clip-mode agc PyTorch global norm of 1.0 (old behaviour, always norm), --clip-grad 1.0 PyTorch value clipping of 10, --clip-grad 10. --clip-mode value AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet. Feb 12, 2021 Update Normalization-Free nets to include new NFNet-F ( https://arxiv.org/abs/2102.06171 ) model defs Feb 10, 2021 More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks') GPU-Efficient-Networks ( https://github.com/idstcv/GPU-Efficient-Networks ), impl in byobnet.py RepVGG ( https://github.com/DingXiaoH/RepVGG ), impl in byobnet.py classic VGG (from torchvision, impl in vgg ) Refinements to normalizer layer arg handling and normalizer+act layer handling in some models Default AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with --channels-last and --torchscript model training, APEX does not. Fix a few bugs introduced since last pypi release Feb 8, 2021 Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352. ecaresnet26t - 79.88 top-1 @ 320x320, 79.08 @ 256x256 ecaresnet50t - 82.35 top-1 @ 320x320, 81.52 @ 256x256 ecaresnet269d - 84.93 top-1 @ 352x352, 84.87 @ 320x320 Remove separate tiered ( t ) vs tiered_narrow ( tn ) ResNet model defs, all tn changed to t and t models removed ( seresnext26t_32x4d only model w/ weights that was removed). Support model default_cfgs with separate train vs test resolution test_input_size and remove extra _320 suffix ResNet model defs that were just for test. Jan 30, 2021 Add initial \"Normalization Free\" NF-RegNet-B* and NF-ResNet model definitions based on paper Jan 25, 2021 Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https://github.com/google-research/big_transfer Add official R50+ViT-B/16 hybrid models + weights from https://github.com/google-research/vision_transformer ImageNet-21k ViT weights are added w/ model defs and representation layer (pre logits) support NOTE: ImageNet-21k classifier heads were zero'd in original weights, they are only useful for transfer learning Add model defs and weights for DeiT Vision Transformer models from https://github.com/facebookresearch/deit Refactor dataset classes into ImageDataset/IterableImageDataset + dataset specific parser classes Add Tensorflow-Datasets (TFDS) wrapper to allow use of TFDS image classification sets with train script Ex: train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2 Add improved .tar dataset parser that reads images from .tar, folder of .tar files, or .tar within .tar Run validation on full ImageNet-21k directly from tar w/ BiT model: validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp Models in this update should be stable w/ possible exception of ViT/BiT, possibility of some regressions with train/val scripts and dataset handling Jan 3, 2021 Add SE-ResNet-152D weights 256x256 val, 0.94 crop top-1 - 83.75 320x320 val, 1.0 crop - 84.36 Update results files Dec 18, 2020 Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256 256x256 val, 0.94 crop (top-1) - 101D (82.33), 152D (83.08), 200D (83.25) 288x288 val, 1.0 crop - 101D (82.64), 152D (83.48), 200D (83.76) 320x320 val, 1.0 crop - 101D (83.00), 152D (83.66), 200D (84.01) Dec 7, 2020 Simplify EMA module (ModelEmaV2), compatible with fully torchscripted models Misc fixes for SiLU ONNX export, default_cfg missing from Feature extraction models, Linear layer w/ AMP + torchscript PyPi release @ 0.3.2 (needed by EfficientDet) Oct 30, 2020 Test with PyTorch 1.7 and fix a small top-n metric view vs reshape issue. Convert newly added 224x224 Vision Transformer weights from official JAX repo. 81.8 top-1 for B/16, 83.1 L/16. Support PyTorch 1.7 optimized, native SiLU (aka Swish) activation. Add mapping to 'silu' name, custom swish will eventually be deprecated. Fix regression for loading pretrained classifier via direct model entrypoint functions. Didn't impact create_model() factory usage. PyPi release @ 0.3.0 version! Oct 26, 2020 Update Vision Transformer models to be compatible with official code release at https://github.com/google-research/vision_transformer Add Vision Transformer weights (ImageNet-21k pretrain) for 384x384 base and large models converted from official jax impl ViT-B/16 - 84.2 ViT-B/32 - 81.7 ViT-L/16 - 85.2 ViT-L/32 - 81.5 Oct 21, 2020 Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79.35 for 'base'. Thanks to Christof for training the base model w/ lots of GPUs. Oct 13, 2020 Initial impl of Vision Transformer models. Both patch and hybrid (CNN backbone) variants. Currently trying to train... Adafactor and AdaHessian (FP32 only, no AMP) optimizers EdgeTPU-M ( efficientnet_em ) model trained in PyTorch, 79.3 top-1 Pip release, doc updates pending a few more changes... Sept 18, 2020 New ResNet 'D' weights. 72.7 (top-1) ResNet-18-D, 77.1 ResNet-34-D, 80.5 ResNet-50-D Added a few untrained defs for other ResNet models (66D, 101D, 152D, 200/200D) Sept 3, 2020 New weights Wide-ResNet50 - 81.5 top-1 (vs 78.5 torchvision) SEResNeXt50-32x4d - 81.3 top-1 (vs 79.1 cadene) Support for native Torch AMP and channels_last memory format added to train/validate scripts ( --channels-last , --native-amp vs --apex-amp ) Models tested with channels_last on latest NGC 20.08 container. AdaptiveAvgPool in attn layers changed to mean((2,3)) to work around bug with NHWC kernel. Aug 12, 2020 New/updated weights from training experiments EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp) RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver) CSPResNet50 - 79.6 top-1 (76.6 from official ver) Add CutMix integrated w/ Mixup. See pull request for some usage examples Some fixes for using pretrained weights with in_chans != 3 on several models. Aug 5, 2020 Universal feature extraction, new models, new weights, new test sets. All models support the features_only=True argument for create_model call to return a network that extracts features from the deepest layer at each stride. New models CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet ReXNet (Modified Aligned) Xception41/65/71 (a proper port of TF models) New trained weights SEResNet50 - 80.3 top-1 CSPDarkNet53 - 80.1 top-1 CSPResNeXt50 - 80.0 top-1 DPN68b - 79.2 top-1 EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by @hal-314 ) Add 'real' labels for ImageNet and ImageNet-Renditions test set, see results/README.md Test set ranking/top-n diff script by @KushajveerSingh Train script and loader/transform tweaks to punch through more aug arguments README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/ adamp and sgdp optimizers added by @hellbell June 11, 2020 Bunch of changes: DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1 Activation factory added along with new activations: select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export) hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish context mgr for setting exportable/scriptable/no_jit states Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call Prep for 0.1.28 pip release May 12, 2020 Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt , paper https://arxiv.org/abs/2004.08955 )) May 3, 2020 Pruned EfficientNet B1, B2, and B3 ( https://arxiv.org/abs/2002.08258 ) contributed by Yonathan Aflalo May 1, 2020 Merged a number of execellent contributions in the ResNet model family over the past month BlurPool2D and resnetblur models initiated by Chris Ha , I trained resnetblur50 to 79.3. TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by mrT23 ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per ( https://arxiv.org/abs/2002.08258 ) by Yonathan Aflalo 200 pretrained models in total now with updated results csv in results folder","title":"Recent Changes"},{"location":"changes/#recent-changes","text":"","title":"Recent Changes"},{"location":"changes/#march-7-2021","text":"First 0.4.x PyPi release w/ NFNets (& related), ByoB (GPU-Efficient, RepVGG, etc). Change feature extraction for pre-activation nets (NFNets, ResNetV2) to return features before activation.","title":"March 7, 2021"},{"location":"changes/#feb-18-2021","text":"Add pretrained weights and model variants for NFNet-F* models from DeepMind Haiku impl . Models are prefixed with dm_ . They require SAME padding conv, skipinit enabled, and activation gains applied in act fn. These models are big, expect to run out of GPU memory. With the GELU activiation + other options, they are roughly \u00bd the inference speed of my SiLU PyTorch optimized s variants. Original model results are based on pre-processing that is not the same as all other models so you'll see different results in the results csv (once updated). Matching the original pre-processing as closely as possible I get these results: dm_nfnet_f6 - 86.352 dm_nfnet_f5 - 86.100 dm_nfnet_f4 - 85.834 dm_nfnet_f3 - 85.676 dm_nfnet_f2 - 85.178 dm_nfnet_f1 - 84.696 dm_nfnet_f0 - 83.464","title":"Feb 18, 2021"},{"location":"changes/#feb-16-2021","text":"Add Adaptive Gradient Clipping (AGC) as per https://arxiv.org/abs/2102.06171 . Integrated w/ PyTorch gradient clipping via mode arg that defaults to prev 'norm' mode. For backward arg compat, clip-grad arg must be specified to enable when using train.py. AGC w/ default clipping factor --clip-grad .01 --clip-mode agc PyTorch global norm of 1.0 (old behaviour, always norm), --clip-grad 1.0 PyTorch value clipping of 10, --clip-grad 10. --clip-mode value AGC performance is definitely sensitive to the clipping factor. More experimentation needed to determine good values for smaller batch sizes and optimizers besides those in paper. So far I've found .001-.005 is necessary for stable RMSProp training w/ NFNet/NF-ResNet.","title":"Feb 16, 2021"},{"location":"changes/#feb-12-2021","text":"Update Normalization-Free nets to include new NFNet-F ( https://arxiv.org/abs/2102.06171 ) model defs","title":"Feb 12, 2021"},{"location":"changes/#feb-10-2021","text":"More model archs, incl a flexible ByobNet backbone ('Bring-your-own-blocks') GPU-Efficient-Networks ( https://github.com/idstcv/GPU-Efficient-Networks ), impl in byobnet.py RepVGG ( https://github.com/DingXiaoH/RepVGG ), impl in byobnet.py classic VGG (from torchvision, impl in vgg ) Refinements to normalizer layer arg handling and normalizer+act layer handling in some models Default AMP mode changed to native PyTorch AMP instead of APEX. Issues not being fixed with APEX. Native works with --channels-last and --torchscript model training, APEX does not. Fix a few bugs introduced since last pypi release","title":"Feb 10, 2021"},{"location":"changes/#feb-8-2021","text":"Add several ResNet weights with ECA attention. 26t & 50t trained @ 256, test @ 320. 269d train @ 256, fine-tune @320, test @ 352. ecaresnet26t - 79.88 top-1 @ 320x320, 79.08 @ 256x256 ecaresnet50t - 82.35 top-1 @ 320x320, 81.52 @ 256x256 ecaresnet269d - 84.93 top-1 @ 352x352, 84.87 @ 320x320 Remove separate tiered ( t ) vs tiered_narrow ( tn ) ResNet model defs, all tn changed to t and t models removed ( seresnext26t_32x4d only model w/ weights that was removed). Support model default_cfgs with separate train vs test resolution test_input_size and remove extra _320 suffix ResNet model defs that were just for test.","title":"Feb 8, 2021"},{"location":"changes/#jan-30-2021","text":"Add initial \"Normalization Free\" NF-RegNet-B* and NF-ResNet model definitions based on paper","title":"Jan 30, 2021"},{"location":"changes/#jan-25-2021","text":"Add ResNetV2 Big Transfer (BiT) models w/ ImageNet-1k and 21k weights from https://github.com/google-research/big_transfer Add official R50+ViT-B/16 hybrid models + weights from https://github.com/google-research/vision_transformer ImageNet-21k ViT weights are added w/ model defs and representation layer (pre logits) support NOTE: ImageNet-21k classifier heads were zero'd in original weights, they are only useful for transfer learning Add model defs and weights for DeiT Vision Transformer models from https://github.com/facebookresearch/deit Refactor dataset classes into ImageDataset/IterableImageDataset + dataset specific parser classes Add Tensorflow-Datasets (TFDS) wrapper to allow use of TFDS image classification sets with train script Ex: train.py /data/tfds --dataset tfds/oxford_iiit_pet --val-split test --model resnet50 -b 256 --amp --num-classes 37 --opt adamw --lr 3e-4 --weight-decay .001 --pretrained -j 2 Add improved .tar dataset parser that reads images from .tar, folder of .tar files, or .tar within .tar Run validation on full ImageNet-21k directly from tar w/ BiT model: validate.py /data/fall11_whole.tar --model resnetv2_50x1_bitm_in21k --amp Models in this update should be stable w/ possible exception of ViT/BiT, possibility of some regressions with train/val scripts and dataset handling","title":"Jan 25, 2021"},{"location":"changes/#jan-3-2021","text":"Add SE-ResNet-152D weights 256x256 val, 0.94 crop top-1 - 83.75 320x320 val, 1.0 crop - 84.36 Update results files","title":"Jan 3, 2021"},{"location":"changes/#dec-18-2020","text":"Add ResNet-101D, ResNet-152D, and ResNet-200D weights trained @ 256x256 256x256 val, 0.94 crop (top-1) - 101D (82.33), 152D (83.08), 200D (83.25) 288x288 val, 1.0 crop - 101D (82.64), 152D (83.48), 200D (83.76) 320x320 val, 1.0 crop - 101D (83.00), 152D (83.66), 200D (84.01)","title":"Dec 18, 2020"},{"location":"changes/#dec-7-2020","text":"Simplify EMA module (ModelEmaV2), compatible with fully torchscripted models Misc fixes for SiLU ONNX export, default_cfg missing from Feature extraction models, Linear layer w/ AMP + torchscript PyPi release @ 0.3.2 (needed by EfficientDet)","title":"Dec 7, 2020"},{"location":"changes/#oct-30-2020","text":"Test with PyTorch 1.7 and fix a small top-n metric view vs reshape issue. Convert newly added 224x224 Vision Transformer weights from official JAX repo. 81.8 top-1 for B/16, 83.1 L/16. Support PyTorch 1.7 optimized, native SiLU (aka Swish) activation. Add mapping to 'silu' name, custom swish will eventually be deprecated. Fix regression for loading pretrained classifier via direct model entrypoint functions. Didn't impact create_model() factory usage. PyPi release @ 0.3.0 version!","title":"Oct 30, 2020"},{"location":"changes/#oct-26-2020","text":"Update Vision Transformer models to be compatible with official code release at https://github.com/google-research/vision_transformer Add Vision Transformer weights (ImageNet-21k pretrain) for 384x384 base and large models converted from official jax impl ViT-B/16 - 84.2 ViT-B/32 - 81.7 ViT-L/16 - 85.2 ViT-L/32 - 81.5","title":"Oct 26, 2020"},{"location":"changes/#oct-21-2020","text":"Weights added for Vision Transformer (ViT) models. 77.86 top-1 for 'small' and 79.35 for 'base'. Thanks to Christof for training the base model w/ lots of GPUs.","title":"Oct 21, 2020"},{"location":"changes/#oct-13-2020","text":"Initial impl of Vision Transformer models. Both patch and hybrid (CNN backbone) variants. Currently trying to train... Adafactor and AdaHessian (FP32 only, no AMP) optimizers EdgeTPU-M ( efficientnet_em ) model trained in PyTorch, 79.3 top-1 Pip release, doc updates pending a few more changes...","title":"Oct 13, 2020"},{"location":"changes/#sept-18-2020","text":"New ResNet 'D' weights. 72.7 (top-1) ResNet-18-D, 77.1 ResNet-34-D, 80.5 ResNet-50-D Added a few untrained defs for other ResNet models (66D, 101D, 152D, 200/200D)","title":"Sept 18, 2020"},{"location":"changes/#sept-3-2020","text":"New weights Wide-ResNet50 - 81.5 top-1 (vs 78.5 torchvision) SEResNeXt50-32x4d - 81.3 top-1 (vs 79.1 cadene) Support for native Torch AMP and channels_last memory format added to train/validate scripts ( --channels-last , --native-amp vs --apex-amp ) Models tested with channels_last on latest NGC 20.08 container. AdaptiveAvgPool in attn layers changed to mean((2,3)) to work around bug with NHWC kernel.","title":"Sept 3, 2020"},{"location":"changes/#aug-12-2020","text":"New/updated weights from training experiments EfficientNet-B3 - 82.1 top-1 (vs 81.6 for official with AA and 81.9 for AdvProp) RegNetY-3.2GF - 82.0 top-1 (78.9 from official ver) CSPResNet50 - 79.6 top-1 (76.6 from official ver) Add CutMix integrated w/ Mixup. See pull request for some usage examples Some fixes for using pretrained weights with in_chans != 3 on several models.","title":"Aug 12, 2020"},{"location":"changes/#aug-5-2020","text":"Universal feature extraction, new models, new weights, new test sets. All models support the features_only=True argument for create_model call to return a network that extracts features from the deepest layer at each stride. New models CSPResNet, CSPResNeXt, CSPDarkNet, DarkNet ReXNet (Modified Aligned) Xception41/65/71 (a proper port of TF models) New trained weights SEResNet50 - 80.3 top-1 CSPDarkNet53 - 80.1 top-1 CSPResNeXt50 - 80.0 top-1 DPN68b - 79.2 top-1 EfficientNet-Lite0 (non-TF ver) - 75.5 (submitted by @hal-314 ) Add 'real' labels for ImageNet and ImageNet-Renditions test set, see results/README.md Test set ranking/top-n diff script by @KushajveerSingh Train script and loader/transform tweaks to punch through more aug arguments README and documentation overhaul. See initial (WIP) documentation at https://rwightman.github.io/pytorch-image-models/ adamp and sgdp optimizers added by @hellbell","title":"Aug 5, 2020"},{"location":"changes/#june-11-2020","text":"Bunch of changes: DenseNet models updated with memory efficient addition from torchvision (fixed a bug), blur pooling and deep stem additions VoVNet V1 and V2 models added, 39 V2 variant (ese_vovnet_39b) trained to 79.3 top-1 Activation factory added along with new activations: select act at model creation time for more flexibility in using activations compatible with scripting or tracing (ONNX export) hard_mish (experimental) added with memory-efficient grad, along with ME hard_swish context mgr for setting exportable/scriptable/no_jit states Norm + Activation combo layers added with initial trial support in DenseNet and VoVNet along with impl of EvoNorm and InplaceAbn wrapper that fit the interface Torchscript works for all but two of the model types as long as using Pytorch 1.5+, tests added for this Some import cleanup and classifier reset changes, all models will have classifier reset to nn.Identity on reset_classifer(0) call Prep for 0.1.28 pip release","title":"June 11, 2020"},{"location":"changes/#may-12-2020","text":"Add ResNeSt models (code adapted from https://github.com/zhanghang1989/ResNeSt , paper https://arxiv.org/abs/2004.08955 ))","title":"May 12, 2020"},{"location":"changes/#may-3-2020","text":"Pruned EfficientNet B1, B2, and B3 ( https://arxiv.org/abs/2002.08258 ) contributed by Yonathan Aflalo","title":"May 3, 2020"},{"location":"changes/#may-1-2020","text":"Merged a number of execellent contributions in the ResNet model family over the past month BlurPool2D and resnetblur models initiated by Chris Ha , I trained resnetblur50 to 79.3. TResNet models and SpaceToDepth, AntiAliasDownsampleLayer layers by mrT23 ecaresnet (50d, 101d, light) models and two pruned variants using pruning as per ( https://arxiv.org/abs/2002.08258 ) by Yonathan Aflalo 200 pretrained models in total now with updated results csv in results folder","title":"May 1, 2020"},{"location":"feature_extraction/","text":"Feature Extraction All of the models in timm have consistent mechanisms for obtaining various types of features from the model for tasks besides classification. Penultimate Layer Features (Pre-Classifier Features) The features from the penultimate model layer can be obtained in several ways without requiring model surgery (although feel free to do surgery). One must first decide if they want pooled or un-pooled features. Unpooled There are three ways to obtain unpooled features. Without modifying the network, one can call model.forward_features(input) on any model instead of the usual model(input) . This will bypass the head classifier and global pooling for networks. If one wants to explicitly modify the network to return unpooled features, they can either create the model without a classifier and pooling, or remove it later. Both paths remove the parameters associated with the classifier from the network. forward_features() import torch import timm m = timm . create_model ( 'xception41' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Original shape: {o.shape}' ) o = m . forward_features ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Unpooled shape: {o.shape}' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 2048, 10, 10]) Create with no classifier and pooling import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 , global_pool = '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: {o.shape}' ) Output: Unpooled shape: torch.Size([2, 2048, 7, 7]) Remove it later import torch import timm m = timm . create_model ( 'densenet121' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: {o.shape}' ) m . reset_classifier ( 0 , '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: {o.shape}' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 1024, 7, 7]) Pooled To modify the network to return pooled features, one can use forward_features() and pool/flatten the result themselves, or modify the network like above but keep pooling intact. Create with no classifier import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: {o.shape}' ) Output: Pooled shape: torch.Size([2, 2048]) Remove it later import torch import timm m = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: {o.shape}' ) m . reset_classifier ( 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: {o.shape}' ) Output: Pooled shape: torch.Size([2, 1024]) Multi-scale Feature Maps (Feature Pyramid) Object detection, segmentation, keypoint, and a variety of dense pixel tasks require access to feature maps from the backbone network at multiple scales. This is often done by modifying the original classification network. Since each network varies quite a bit in structure, it's not uncommon to see only a few backbones supported in any given obj detection or segmentation library. timm allows a consistent interface for creating any of the included models as feature backbones that output feature maps for selected levels. A feature backbone can be created by adding the argument features_only=True to any create_model call. By default 5 strides will be output from most models (not all have that many), with the first starting at 2 (some start at 1 or 4). Create a feature map extraction model import torch import timm m = timm . create_model ( 'resnest26d' , features_only = True , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: torch.Size([2, 64, 112, 112]) torch.Size([2, 256, 56, 56]) torch.Size([2, 512, 28, 28]) torch.Size([2, 1024, 14, 14]) torch.Size([2, 2048, 7, 7]) Query the feature information After a feature backbone has been created, it can be queried to provide channel or resolution reduction information to the downstream heads without requiring static config or hardcoded constants. The .feature_info attribute is a class encapsulating the information about the feature extraction points. import torch import timm m = timm . create_model ( 'regnety_032' , features_only = True , pretrained = True ) print ( f 'Feature channels: {m.feature_info.channels()}' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: Feature channels: [32, 72, 216, 576, 1512] torch.Size([2, 32, 112, 112]) torch.Size([2, 72, 56, 56]) torch.Size([2, 216, 28, 28]) torch.Size([2, 576, 14, 14]) torch.Size([2, 1512, 7, 7]) Select specific feature levels or limit the stride There are to additional creation arguments impacting the output features. out_indices selects which indices to output output_stride limits the feature output stride of the network (also works in classification mode BTW) out_indices is supported by all models, but not all models have the same index to feature stride mapping. Look at the code or check feature_info to compare. The out indices generally correspond to the C(i+1)th feature level (a 2^(i+1) reduction). For most models, index 0 is the stride 2 features, and index 4 is stride 32. output_stride is achieved by converting layers to use dilated convolutions. Doing so is not always straightforward, some networks only support output_stride=32 . import torch import timm m = timm . create_model ( 'ecaresnet101d' , features_only = True , output_stride = 8 , out_indices = ( 2 , 4 ), pretrained = True ) print ( f 'Feature channels: {m.feature_info.channels()}' ) print ( f 'Feature reduction: {m.feature_info.reduction()}' ) o = m ( torch . randn ( 2 , 3 , 320 , 320 )) for x in o : print ( x . shape ) Output: Feature channels: [512, 2048] Feature reduction: [8, 8] torch.Size([2, 512, 40, 40]) torch.Size([2, 2048, 40, 40])","title":"Feature Extraction"},{"location":"feature_extraction/#feature-extraction","text":"All of the models in timm have consistent mechanisms for obtaining various types of features from the model for tasks besides classification.","title":"Feature Extraction"},{"location":"feature_extraction/#penultimate-layer-features-pre-classifier-features","text":"The features from the penultimate model layer can be obtained in several ways without requiring model surgery (although feel free to do surgery). One must first decide if they want pooled or un-pooled features.","title":"Penultimate Layer Features (Pre-Classifier Features)"},{"location":"feature_extraction/#unpooled","text":"There are three ways to obtain unpooled features. Without modifying the network, one can call model.forward_features(input) on any model instead of the usual model(input) . This will bypass the head classifier and global pooling for networks. If one wants to explicitly modify the network to return unpooled features, they can either create the model without a classifier and pooling, or remove it later. Both paths remove the parameters associated with the classifier from the network.","title":"Unpooled"},{"location":"feature_extraction/#forward_features","text":"import torch import timm m = timm . create_model ( 'xception41' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Original shape: {o.shape}' ) o = m . forward_features ( torch . randn ( 2 , 3 , 299 , 299 )) print ( f 'Unpooled shape: {o.shape}' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 2048, 10, 10])","title":"forward_features()"},{"location":"feature_extraction/#create-with-no-classifier-and-pooling","text":"import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 , global_pool = '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: {o.shape}' ) Output: Unpooled shape: torch.Size([2, 2048, 7, 7])","title":"Create with no classifier and pooling"},{"location":"feature_extraction/#remove-it-later","text":"import torch import timm m = timm . create_model ( 'densenet121' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: {o.shape}' ) m . reset_classifier ( 0 , '' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Unpooled shape: {o.shape}' ) Output: Original shape: torch.Size([2, 1000]) Unpooled shape: torch.Size([2, 1024, 7, 7])","title":"Remove it later"},{"location":"feature_extraction/#pooled","text":"To modify the network to return pooled features, one can use forward_features() and pool/flatten the result themselves, or modify the network like above but keep pooling intact.","title":"Pooled"},{"location":"feature_extraction/#create-with-no-classifier","text":"import torch import timm m = timm . create_model ( 'resnet50' , pretrained = True , num_classes = 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: {o.shape}' ) Output: Pooled shape: torch.Size([2, 2048])","title":"Create with no classifier"},{"location":"feature_extraction/#remove-it-later_1","text":"import torch import timm m = timm . create_model ( 'ese_vovnet19b_dw' , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Original shape: {o.shape}' ) m . reset_classifier ( 0 ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) print ( f 'Pooled shape: {o.shape}' ) Output: Pooled shape: torch.Size([2, 1024])","title":"Remove it later"},{"location":"feature_extraction/#multi-scale-feature-maps-feature-pyramid","text":"Object detection, segmentation, keypoint, and a variety of dense pixel tasks require access to feature maps from the backbone network at multiple scales. This is often done by modifying the original classification network. Since each network varies quite a bit in structure, it's not uncommon to see only a few backbones supported in any given obj detection or segmentation library. timm allows a consistent interface for creating any of the included models as feature backbones that output feature maps for selected levels. A feature backbone can be created by adding the argument features_only=True to any create_model call. By default 5 strides will be output from most models (not all have that many), with the first starting at 2 (some start at 1 or 4).","title":"Multi-scale Feature Maps (Feature Pyramid)"},{"location":"feature_extraction/#create-a-feature-map-extraction-model","text":"import torch import timm m = timm . create_model ( 'resnest26d' , features_only = True , pretrained = True ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: torch.Size([2, 64, 112, 112]) torch.Size([2, 256, 56, 56]) torch.Size([2, 512, 28, 28]) torch.Size([2, 1024, 14, 14]) torch.Size([2, 2048, 7, 7])","title":"Create a feature map extraction model"},{"location":"feature_extraction/#query-the-feature-information","text":"After a feature backbone has been created, it can be queried to provide channel or resolution reduction information to the downstream heads without requiring static config or hardcoded constants. The .feature_info attribute is a class encapsulating the information about the feature extraction points. import torch import timm m = timm . create_model ( 'regnety_032' , features_only = True , pretrained = True ) print ( f 'Feature channels: {m.feature_info.channels()}' ) o = m ( torch . randn ( 2 , 3 , 224 , 224 )) for x in o : print ( x . shape ) Output: Feature channels: [32, 72, 216, 576, 1512] torch.Size([2, 32, 112, 112]) torch.Size([2, 72, 56, 56]) torch.Size([2, 216, 28, 28]) torch.Size([2, 576, 14, 14]) torch.Size([2, 1512, 7, 7])","title":"Query the feature information"},{"location":"feature_extraction/#select-specific-feature-levels-or-limit-the-stride","text":"There are to additional creation arguments impacting the output features. out_indices selects which indices to output output_stride limits the feature output stride of the network (also works in classification mode BTW) out_indices is supported by all models, but not all models have the same index to feature stride mapping. Look at the code or check feature_info to compare. The out indices generally correspond to the C(i+1)th feature level (a 2^(i+1) reduction). For most models, index 0 is the stride 2 features, and index 4 is stride 32. output_stride is achieved by converting layers to use dilated convolutions. Doing so is not always straightforward, some networks only support output_stride=32 . import torch import timm m = timm . create_model ( 'ecaresnet101d' , features_only = True , output_stride = 8 , out_indices = ( 2 , 4 ), pretrained = True ) print ( f 'Feature channels: {m.feature_info.channels()}' ) print ( f 'Feature reduction: {m.feature_info.reduction()}' ) o = m ( torch . randn ( 2 , 3 , 320 , 320 )) for x in o : print ( x . shape ) Output: Feature channels: [512, 2048] Feature reduction: [8, 8] torch.Size([2, 512, 40, 40]) torch.Size([2, 2048, 40, 40])","title":"Select specific feature levels or limit the stride"},{"location":"models/","text":"Model Architectures The model architectures included come from a wide variety of sources. Sources, including papers, original impl (\"reference code\") that I rewrote / adapted, and PyTorch impl that I leveraged directly (\"code\") are listed below. Most included models have pretrained weights. The weights are either: from their original sources ported by myself from their original impl in a different framework (e.g. Tensorflow models) trained from scratch using the included training script The validation results for the pretrained weights are here A more exciting view (with pretty pictures) of the models within timm can be found at paperswithcode . Big Transfer ResNetV2 (BiT) [ resnetv2.py ] Paper: Big Transfer (BiT): General Visual Representation Learning - https://arxiv.org/abs/1912.11370 Reference code: https://github.com/google-research/big_transfer Cross-Stage Partial Networks [ cspnet.py ] Paper: CSPNet: A New Backbone that can Enhance Learning Capability of CNN - https://arxiv.org/abs/1911.11929 Reference impl: https://github.com/WongKinYiu/CrossStagePartialNetworks DenseNet [ densenet.py ] Paper: Densely Connected Convolutional Networks - https://arxiv.org/abs/1608.06993 Code: https://github.com/pytorch/vision/tree/master/torchvision/models DLA [ dla.py ] Paper: https://arxiv.org/abs/1707.06484 Code: https://github.com/ucbdrive/dla Dual-Path Networks [ dpn.py ] Paper: Dual Path Networks - https://arxiv.org/abs/1707.01629 My PyTorch code: https://github.com/rwightman/pytorch-dpn-pretrained Reference code: https://github.com/cypw/DPNs GPU-Efficient Networks [ byobnet.py ] Paper: Neural Architecture Design for GPU-Efficient Networks - https://arxiv.org/abs/2006.14090 Reference code: https://github.com/idstcv/GPU-Efficient-Networks HRNet [ hrnet.py ] Paper: Deep High-Resolution Representation Learning for Visual Recognition - https://arxiv.org/abs/1908.07919 Code: https://github.com/HRNet/HRNet-Image-Classification Inception-V3 [ inception_v3.py ] Paper: Rethinking the Inception Architecture for Computer Vision - https://arxiv.org/abs/1512.00567 Code: https://github.com/pytorch/vision/tree/master/torchvision/models Inception-V4 [ inception_v4.py ] Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets Inception-ResNet-V2 [ inception_resnet_v2.py ] Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets NASNet-A [ nasnet.py ] Papers: Learning Transferable Architectures for Scalable Image Recognition - https://arxiv.org/abs/1707.07012 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet PNasNet-5 [ pnasnet.py ] Papers: Progressive Neural Architecture Search - https://arxiv.org/abs/1712.00559 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet EfficientNet [ efficientnet.py ] Papers: EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252 EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665 EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946 EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html MixNet - https://arxiv.org/abs/1907.09595 MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626 MobileNet-V2 - https://arxiv.org/abs/1801.04381 FBNet-C - https://arxiv.org/abs/1812.03443 Single-Path NAS - https://arxiv.org/abs/1904.02877 My PyTorch code: https://github.com/rwightman/gen-efficientnet-pytorch Reference code: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet MobileNet-V3 [ mobilenetv3.py ] Paper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244 Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet RegNet [ regnet.py ] Paper: Designing Network Design Spaces - https://arxiv.org/abs/2003.13678 Reference code: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py RepVGG [ byobnet.py ] Paper: Making VGG-style ConvNets Great Again - https://arxiv.org/abs/2101.03697 Reference code: https://github.com/DingXiaoH/RepVGG ResNet, ResNeXt [ resnet.py ] ResNet (V1B) Paper: Deep Residual Learning for Image Recognition - https://arxiv.org/abs/1512.03385 Code: https://github.com/pytorch/vision/tree/master/torchvision/models ResNeXt Paper: Aggregated Residual Transformations for Deep Neural Networks - https://arxiv.org/abs/1611.05431 Code: https://github.com/pytorch/vision/tree/master/torchvision/models 'Bag of Tricks' / Gluon C, D, E, S ResNet variants Paper: Bag of Tricks for Image Classification with CNNs - https://arxiv.org/abs/1812.01187 Code: https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py Instagram pretrained / ImageNet tuned ResNeXt101 Paper: Exploring the Limits of Weakly Supervised Pretraining - https://arxiv.org/abs/1805.00932 Weights: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet and ResNeXts Paper: Billion-scale semi-supervised learning for image classification - https://arxiv.org/abs/1905.00546 Weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Squeeze-and-Excitation Networks Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: Added to ResNet base, this is current version going forward, old senet.py is being deprecated ECAResNet (ECA-Net) Paper: ECA-Net: Efficient Channel Attention for Deep CNN - https://arxiv.org/abs/1910.03151v4 Code: Added to ResNet base, ECA module contributed by @VRandme, reference https://github.com/BangguWu/ECANet Res2Net [ res2net.py ] Paper: Res2Net: A New Multi-scale Backbone Architecture - https://arxiv.org/abs/1904.01169 Code: https://github.com/gasvn/Res2Net ResNeSt [ resnest.py ] Paper: ResNeSt: Split-Attention Networks - https://arxiv.org/abs/2004.08955 Code: https://github.com/zhanghang1989/ResNeSt ReXNet [ rexnet.py ] Paper: ReXNet: Diminishing Representational Bottleneck on CNN - https://arxiv.org/abs/2007.00992 Code: https://github.com/clovaai/rexnet Selective-Kernel Networks [ sknet.py ] Paper: Selective-Kernel Networks - https://arxiv.org/abs/1903.06586 Code: https://github.com/implus/SKNet , https://github.com/clovaai/assembled-cnn SelecSLS [ selecsls.py ] Paper: XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera - https://arxiv.org/abs/1907.00837 Code: https://github.com/mehtadushy/SelecSLS-Pytorch Squeeze-and-Excitation Networks [ senet.py ] NOTE: I am deprecating this version of the networks, the new ones are part of resnet.py Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: https://github.com/Cadene/pretrained-models.pytorch TResNet [ tresnet.py ] Paper: TResNet: High Performance GPU-Dedicated Architecture - https://arxiv.org/abs/2003.13630 Code: https://github.com/mrT23/TResNet VGG [ vgg.py ] Paper: Very Deep Convolutional Networks For Large-Scale Image Recognition - https://arxiv.org/pdf/1409.1556.pdf Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py Vision Transformer [ vision_transformer.py ] Paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929 Reference code and pretrained weights: https://github.com/google-research/vision_transformer VovNet V2 and V1 [ vovnet.py ] Paper: CenterMask : Real-Time Anchor-Free Instance Segmentation - https://arxiv.org/abs/1911.06667 Reference code: https://github.com/youngwanLEE/vovnet-detectron2 Xception [ xception.py ] Paper: Xception: Deep Learning with Depthwise Separable Convolutions - https://arxiv.org/abs/1610.02357 Code: https://github.com/Cadene/pretrained-models.pytorch Xception (Modified Aligned, Gluon) [ gluon_xception.py ] Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo , https://github.com/jfzhang95/pytorch-deeplab-xception/ Xception (Modified Aligned, TF) [ aligned_xception.py ] Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/tensorflow/models/tree/master/research/deeplab","title":"Model Architectures"},{"location":"models/#model-architectures","text":"The model architectures included come from a wide variety of sources. Sources, including papers, original impl (\"reference code\") that I rewrote / adapted, and PyTorch impl that I leveraged directly (\"code\") are listed below. Most included models have pretrained weights. The weights are either: from their original sources ported by myself from their original impl in a different framework (e.g. Tensorflow models) trained from scratch using the included training script The validation results for the pretrained weights are here A more exciting view (with pretty pictures) of the models within timm can be found at paperswithcode .","title":"Model Architectures"},{"location":"models/#big-transfer-resnetv2-bit-resnetv2py","text":"Paper: Big Transfer (BiT): General Visual Representation Learning - https://arxiv.org/abs/1912.11370 Reference code: https://github.com/google-research/big_transfer","title":"Big Transfer ResNetV2 (BiT) [resnetv2.py]"},{"location":"models/#cross-stage-partial-networks-cspnetpy","text":"Paper: CSPNet: A New Backbone that can Enhance Learning Capability of CNN - https://arxiv.org/abs/1911.11929 Reference impl: https://github.com/WongKinYiu/CrossStagePartialNetworks","title":"Cross-Stage Partial Networks [cspnet.py]"},{"location":"models/#densenet-densenetpy","text":"Paper: Densely Connected Convolutional Networks - https://arxiv.org/abs/1608.06993 Code: https://github.com/pytorch/vision/tree/master/torchvision/models","title":"DenseNet [densenet.py]"},{"location":"models/#dla-dlapy","text":"Paper: https://arxiv.org/abs/1707.06484 Code: https://github.com/ucbdrive/dla","title":"DLA [dla.py]"},{"location":"models/#dual-path-networks-dpnpy","text":"Paper: Dual Path Networks - https://arxiv.org/abs/1707.01629 My PyTorch code: https://github.com/rwightman/pytorch-dpn-pretrained Reference code: https://github.com/cypw/DPNs","title":"Dual-Path Networks [dpn.py]"},{"location":"models/#gpu-efficient-networks-byobnetpy","text":"Paper: Neural Architecture Design for GPU-Efficient Networks - https://arxiv.org/abs/2006.14090 Reference code: https://github.com/idstcv/GPU-Efficient-Networks","title":"GPU-Efficient Networks [byobnet.py]"},{"location":"models/#hrnet-hrnetpy","text":"Paper: Deep High-Resolution Representation Learning for Visual Recognition - https://arxiv.org/abs/1908.07919 Code: https://github.com/HRNet/HRNet-Image-Classification","title":"HRNet [hrnet.py]"},{"location":"models/#inception-v3-inception_v3py","text":"Paper: Rethinking the Inception Architecture for Computer Vision - https://arxiv.org/abs/1512.00567 Code: https://github.com/pytorch/vision/tree/master/torchvision/models","title":"Inception-V3 [inception_v3.py]"},{"location":"models/#inception-v4-inception_v4py","text":"Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets","title":"Inception-V4 [inception_v4.py]"},{"location":"models/#inception-resnet-v2-inception_resnet_v2py","text":"Paper: Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning - https://arxiv.org/abs/1602.07261 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets","title":"Inception-ResNet-V2 [inception_resnet_v2.py]"},{"location":"models/#nasnet-a-nasnetpy","text":"Papers: Learning Transferable Architectures for Scalable Image Recognition - https://arxiv.org/abs/1707.07012 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet","title":"NASNet-A [nasnet.py]"},{"location":"models/#pnasnet-5-pnasnetpy","text":"Papers: Progressive Neural Architecture Search - https://arxiv.org/abs/1712.00559 Code: https://github.com/Cadene/pretrained-models.pytorch Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/nasnet","title":"PNasNet-5 [pnasnet.py]"},{"location":"models/#efficientnet-efficientnetpy","text":"Papers: EfficientNet NoisyStudent (B0-B7, L2) - https://arxiv.org/abs/1911.04252 EfficientNet AdvProp (B0-B8) - https://arxiv.org/abs/1911.09665 EfficientNet (B0-B7) - https://arxiv.org/abs/1905.11946 EfficientNet-EdgeTPU (S, M, L) - https://ai.googleblog.com/2019/08/efficientnet-edgetpu-creating.html MixNet - https://arxiv.org/abs/1907.09595 MNASNet B1, A1 (Squeeze-Excite), and Small - https://arxiv.org/abs/1807.11626 MobileNet-V2 - https://arxiv.org/abs/1801.04381 FBNet-C - https://arxiv.org/abs/1812.03443 Single-Path NAS - https://arxiv.org/abs/1904.02877 My PyTorch code: https://github.com/rwightman/gen-efficientnet-pytorch Reference code: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet","title":"EfficientNet [efficientnet.py]"},{"location":"models/#mobilenet-v3-mobilenetv3py","text":"Paper: Searching for MobileNetV3 - https://arxiv.org/abs/1905.02244 Reference code: https://github.com/tensorflow/models/tree/master/research/slim/nets/mobilenet","title":"MobileNet-V3 [mobilenetv3.py]"},{"location":"models/#regnet-regnetpy","text":"Paper: Designing Network Design Spaces - https://arxiv.org/abs/2003.13678 Reference code: https://github.com/facebookresearch/pycls/blob/master/pycls/models/regnet.py","title":"RegNet [regnet.py]"},{"location":"models/#repvgg-byobnetpy","text":"Paper: Making VGG-style ConvNets Great Again - https://arxiv.org/abs/2101.03697 Reference code: https://github.com/DingXiaoH/RepVGG","title":"RepVGG [byobnet.py]"},{"location":"models/#resnet-resnext-resnetpy","text":"ResNet (V1B) Paper: Deep Residual Learning for Image Recognition - https://arxiv.org/abs/1512.03385 Code: https://github.com/pytorch/vision/tree/master/torchvision/models ResNeXt Paper: Aggregated Residual Transformations for Deep Neural Networks - https://arxiv.org/abs/1611.05431 Code: https://github.com/pytorch/vision/tree/master/torchvision/models 'Bag of Tricks' / Gluon C, D, E, S ResNet variants Paper: Bag of Tricks for Image Classification with CNNs - https://arxiv.org/abs/1812.01187 Code: https://github.com/dmlc/gluon-cv/blob/master/gluoncv/model_zoo/resnetv1b.py Instagram pretrained / ImageNet tuned ResNeXt101 Paper: Exploring the Limits of Weakly Supervised Pretraining - https://arxiv.org/abs/1805.00932 Weights: https://pytorch.org/hub/facebookresearch_WSL-Images_resnext (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet and ResNeXts Paper: Billion-scale semi-supervised learning for image classification - https://arxiv.org/abs/1905.00546 Weights: https://github.com/facebookresearch/semi-supervised-ImageNet1K-models (NOTE: CC BY-NC 4.0 License, NOT commercial friendly) Squeeze-and-Excitation Networks Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: Added to ResNet base, this is current version going forward, old senet.py is being deprecated ECAResNet (ECA-Net) Paper: ECA-Net: Efficient Channel Attention for Deep CNN - https://arxiv.org/abs/1910.03151v4 Code: Added to ResNet base, ECA module contributed by @VRandme, reference https://github.com/BangguWu/ECANet","title":"ResNet, ResNeXt [resnet.py]"},{"location":"models/#res2net-res2netpy","text":"Paper: Res2Net: A New Multi-scale Backbone Architecture - https://arxiv.org/abs/1904.01169 Code: https://github.com/gasvn/Res2Net","title":"Res2Net [res2net.py]"},{"location":"models/#resnest-resnestpy","text":"Paper: ResNeSt: Split-Attention Networks - https://arxiv.org/abs/2004.08955 Code: https://github.com/zhanghang1989/ResNeSt","title":"ResNeSt [resnest.py]"},{"location":"models/#rexnet-rexnetpy","text":"Paper: ReXNet: Diminishing Representational Bottleneck on CNN - https://arxiv.org/abs/2007.00992 Code: https://github.com/clovaai/rexnet","title":"ReXNet [rexnet.py]"},{"location":"models/#selective-kernel-networks-sknetpy","text":"Paper: Selective-Kernel Networks - https://arxiv.org/abs/1903.06586 Code: https://github.com/implus/SKNet , https://github.com/clovaai/assembled-cnn","title":"Selective-Kernel Networks [sknet.py]"},{"location":"models/#selecsls-selecslspy","text":"Paper: XNect: Real-time Multi-Person 3D Motion Capture with a Single RGB Camera - https://arxiv.org/abs/1907.00837 Code: https://github.com/mehtadushy/SelecSLS-Pytorch","title":"SelecSLS [selecsls.py]"},{"location":"models/#squeeze-and-excitation-networks-senetpy","text":"NOTE: I am deprecating this version of the networks, the new ones are part of resnet.py Paper: Squeeze-and-Excitation Networks - https://arxiv.org/abs/1709.01507 Code: https://github.com/Cadene/pretrained-models.pytorch","title":"Squeeze-and-Excitation Networks [senet.py]"},{"location":"models/#tresnet-tresnetpy","text":"Paper: TResNet: High Performance GPU-Dedicated Architecture - https://arxiv.org/abs/2003.13630 Code: https://github.com/mrT23/TResNet","title":"TResNet [tresnet.py]"},{"location":"models/#vgg-vggpy","text":"Paper: Very Deep Convolutional Networks For Large-Scale Image Recognition - https://arxiv.org/pdf/1409.1556.pdf Reference code: https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py","title":"VGG [vgg.py]"},{"location":"models/#vision-transformer-vision_transformerpy","text":"Paper: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale - https://arxiv.org/abs/2010.11929 Reference code and pretrained weights: https://github.com/google-research/vision_transformer","title":"Vision Transformer [vision_transformer.py]"},{"location":"models/#vovnet-v2-and-v1-vovnetpy","text":"Paper: CenterMask : Real-Time Anchor-Free Instance Segmentation - https://arxiv.org/abs/1911.06667 Reference code: https://github.com/youngwanLEE/vovnet-detectron2","title":"VovNet V2 and V1 [vovnet.py]"},{"location":"models/#xception-xceptionpy","text":"Paper: Xception: Deep Learning with Depthwise Separable Convolutions - https://arxiv.org/abs/1610.02357 Code: https://github.com/Cadene/pretrained-models.pytorch","title":"Xception [xception.py]"},{"location":"models/#xception-modified-aligned-gluon-gluon_xceptionpy","text":"Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/dmlc/gluon-cv/tree/master/gluoncv/model_zoo , https://github.com/jfzhang95/pytorch-deeplab-xception/","title":"Xception (Modified Aligned, Gluon) [gluon_xception.py]"},{"location":"models/#xception-modified-aligned-tf-aligned_xceptionpy","text":"Paper: Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation - https://arxiv.org/abs/1802.02611 Reference code: https://github.com/tensorflow/models/tree/master/research/deeplab","title":"Xception (Modified Aligned, TF) [aligned_xception.py]"},{"location":"results/","text":"Results CSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation results for all models with pretrained weights is located in the repository results folder . Self-trained Weights The table below includes ImageNet-1k validation results of model weights that I've trained myself. It is not updated as frequently as the csv results outputs linked above. Model Acc@1 (Err) Acc@5 (Err) Param # (M) Interpolation Image Size efficientnet_b3a 82.242 (17.758) 96.114 (3.886) 12.23 bicubic 320 (1.0 crop) efficientnet_b3 82.076 (17.924) 96.020 (3.980) 12.23 bicubic 300 regnet_32 82.002 (17.998) 95.906 (4.094) 19.44 bicubic 224 skresnext50d_32x4d 81.278 (18.722) 95.366 (4.634) 27.5 bicubic 288 (1.0 crop) seresnext50d_32x4d 81.266 (18.734) 95.620 (4.380) 27.6 bicubic 224 efficientnet_b2a 80.608 (19.392) 95.310 (4.690) 9.11 bicubic 288 (1.0 crop) resnet50d 80.530 (19.470) 95.160 (4.840) 25.6 bicubic 224 mixnet_xl 80.478 (19.522) 94.932 (5.068) 11.90 bicubic 224 efficientnet_b2 80.402 (19.598) 95.076 (4.924) 9.11 bicubic 260 seresnet50 80.274 (19.726) 95.070 (4.930) 28.1 bicubic 224 skresnext50d_32x4d 80.156 (19.844) 94.642 (5.358) 27.5 bicubic 224 cspdarknet53 80.058 (19.942) 95.084 (4.916) 27.6 bicubic 256 cspresnext50 80.040 (19.960) 94.944 (5.056) 20.6 bicubic 224 resnext50_32x4d 79.762 (20.238) 94.600 (5.400) 25 bicubic 224 resnext50d_32x4d 79.674 (20.326) 94.868 (5.132) 25.1 bicubic 224 cspresnet50 79.574 (20.426) 94.712 (5.288) 21.6 bicubic 256 ese_vovnet39b 79.320 (20.680) 94.710 (5.290) 24.6 bicubic 224 resnetblur50 79.290 (20.710) 94.632 (5.368) 25.6 bicubic 224 dpn68b 79.216 (20.784) 94.414 (5.586) 12.6 bicubic 224 resnet50 79.038 (20.962) 94.390 (5.610) 25.6 bicubic 224 mixnet_l 78.976 (21.024 94.184 (5.816) 7.33 bicubic 224 efficientnet_b1 78.692 (21.308) 94.086 (5.914) 7.79 bicubic 240 efficientnet_es 78.066 (21.934) 93.926 (6.074) 5.44 bicubic 224 seresnext26t_32x4d 77.998 (22.002) 93.708 (6.292) 16.8 bicubic 224 seresnext26tn_32x4d 77.986 (22.014) 93.746 (6.254) 16.8 bicubic 224 efficientnet_b0 77.698 (22.302) 93.532 (6.468) 5.29 bicubic 224 seresnext26d_32x4d 77.602 (22.398) 93.608 (6.392) 16.8 bicubic 224 mobilenetv2_120d 77.294 (22.706 93.502 (6.498) 5.8 bicubic 224 mixnet_m 77.256 (22.744) 93.418 (6.582) 5.01 bicubic 224 resnet34d 77.116 (22.884) 93.382 (6.618) 21.8 bicubic 224 seresnext26_32x4d 77.104 (22.896) 93.316 (6.684) 16.8 bicubic 224 skresnet34 76.912 (23.088) 93.322 (6.678) 22.2 bicubic 224 ese_vovnet19b_dw 76.798 (23.202) 93.268 (6.732) 6.5 bicubic 224 resnet26d 76.68 (23.32) 93.166 (6.834) 16 bicubic 224 densenetblur121d 76.576 (23.424) 93.190 (6.810) 8.0 bicubic 224 mobilenetv2_140 76.524 (23.476) 92.990 (7.010) 6.1 bicubic 224 mixnet_s 75.988 (24.012) 92.794 (7.206) 4.13 bicubic 224 mobilenetv3_large_100 75.766 (24.234) 92.542 (7.458) 5.5 bicubic 224 mobilenetv3_rw 75.634 (24.366) 92.708 (7.292) 5.5 bicubic 224 mnasnet_a1 75.448 (24.552) 92.604 (7.396) 3.89 bicubic 224 resnet26 75.292 (24.708) 92.57 (7.43) 16 bicubic 224 fbnetc_100 75.124 (24.876) 92.386 (7.614) 5.6 bilinear 224 resnet34 75.110 (24.890) 92.284 (7.716) 22 bilinear 224 mobilenetv2_110d 75.052 (24.948) 92.180 (7.820) 4.5 bicubic 224 seresnet34 74.808 (25.192) 92.124 (7.876) 22 bilinear 224 mnasnet_b1 74.658 (25.342) 92.114 (7.886) 4.38 bicubic 224 spnasnet_100 74.084 (25.916) 91.818 (8.182) 4.42 bilinear 224 skresnet18 73.038 (26.962) 91.168 (8.832) 11.9 bicubic 224 mobilenetv2_100 72.978 (27.022) 91.016 (8.984) 3.5 bicubic 224 resnet18d 72.260 (27.740) 90.696 (9.304) 11.7 bicubic 224 seresnet18 71.742 (28.258) 90.334 (9.666) 11.8 bicubic 224 Ported and Other Weights For weights ported from other deep learning frameworks (Tensorflow, MXNet GluonCV) or copied from other PyTorch sources, please see the full results tables for ImageNet and various OOD test sets at in the results tables . Model code .py files contain links to original sources of models and weights.","title":"Results"},{"location":"results/#results","text":"CSV files containing an ImageNet-1K and out-of-distribution (OOD) test set validation results for all models with pretrained weights is located in the repository results folder .","title":"Results"},{"location":"results/#self-trained-weights","text":"The table below includes ImageNet-1k validation results of model weights that I've trained myself. It is not updated as frequently as the csv results outputs linked above. Model Acc@1 (Err) Acc@5 (Err) Param # (M) Interpolation Image Size efficientnet_b3a 82.242 (17.758) 96.114 (3.886) 12.23 bicubic 320 (1.0 crop) efficientnet_b3 82.076 (17.924) 96.020 (3.980) 12.23 bicubic 300 regnet_32 82.002 (17.998) 95.906 (4.094) 19.44 bicubic 224 skresnext50d_32x4d 81.278 (18.722) 95.366 (4.634) 27.5 bicubic 288 (1.0 crop) seresnext50d_32x4d 81.266 (18.734) 95.620 (4.380) 27.6 bicubic 224 efficientnet_b2a 80.608 (19.392) 95.310 (4.690) 9.11 bicubic 288 (1.0 crop) resnet50d 80.530 (19.470) 95.160 (4.840) 25.6 bicubic 224 mixnet_xl 80.478 (19.522) 94.932 (5.068) 11.90 bicubic 224 efficientnet_b2 80.402 (19.598) 95.076 (4.924) 9.11 bicubic 260 seresnet50 80.274 (19.726) 95.070 (4.930) 28.1 bicubic 224 skresnext50d_32x4d 80.156 (19.844) 94.642 (5.358) 27.5 bicubic 224 cspdarknet53 80.058 (19.942) 95.084 (4.916) 27.6 bicubic 256 cspresnext50 80.040 (19.960) 94.944 (5.056) 20.6 bicubic 224 resnext50_32x4d 79.762 (20.238) 94.600 (5.400) 25 bicubic 224 resnext50d_32x4d 79.674 (20.326) 94.868 (5.132) 25.1 bicubic 224 cspresnet50 79.574 (20.426) 94.712 (5.288) 21.6 bicubic 256 ese_vovnet39b 79.320 (20.680) 94.710 (5.290) 24.6 bicubic 224 resnetblur50 79.290 (20.710) 94.632 (5.368) 25.6 bicubic 224 dpn68b 79.216 (20.784) 94.414 (5.586) 12.6 bicubic 224 resnet50 79.038 (20.962) 94.390 (5.610) 25.6 bicubic 224 mixnet_l 78.976 (21.024 94.184 (5.816) 7.33 bicubic 224 efficientnet_b1 78.692 (21.308) 94.086 (5.914) 7.79 bicubic 240 efficientnet_es 78.066 (21.934) 93.926 (6.074) 5.44 bicubic 224 seresnext26t_32x4d 77.998 (22.002) 93.708 (6.292) 16.8 bicubic 224 seresnext26tn_32x4d 77.986 (22.014) 93.746 (6.254) 16.8 bicubic 224 efficientnet_b0 77.698 (22.302) 93.532 (6.468) 5.29 bicubic 224 seresnext26d_32x4d 77.602 (22.398) 93.608 (6.392) 16.8 bicubic 224 mobilenetv2_120d 77.294 (22.706 93.502 (6.498) 5.8 bicubic 224 mixnet_m 77.256 (22.744) 93.418 (6.582) 5.01 bicubic 224 resnet34d 77.116 (22.884) 93.382 (6.618) 21.8 bicubic 224 seresnext26_32x4d 77.104 (22.896) 93.316 (6.684) 16.8 bicubic 224 skresnet34 76.912 (23.088) 93.322 (6.678) 22.2 bicubic 224 ese_vovnet19b_dw 76.798 (23.202) 93.268 (6.732) 6.5 bicubic 224 resnet26d 76.68 (23.32) 93.166 (6.834) 16 bicubic 224 densenetblur121d 76.576 (23.424) 93.190 (6.810) 8.0 bicubic 224 mobilenetv2_140 76.524 (23.476) 92.990 (7.010) 6.1 bicubic 224 mixnet_s 75.988 (24.012) 92.794 (7.206) 4.13 bicubic 224 mobilenetv3_large_100 75.766 (24.234) 92.542 (7.458) 5.5 bicubic 224 mobilenetv3_rw 75.634 (24.366) 92.708 (7.292) 5.5 bicubic 224 mnasnet_a1 75.448 (24.552) 92.604 (7.396) 3.89 bicubic 224 resnet26 75.292 (24.708) 92.57 (7.43) 16 bicubic 224 fbnetc_100 75.124 (24.876) 92.386 (7.614) 5.6 bilinear 224 resnet34 75.110 (24.890) 92.284 (7.716) 22 bilinear 224 mobilenetv2_110d 75.052 (24.948) 92.180 (7.820) 4.5 bicubic 224 seresnet34 74.808 (25.192) 92.124 (7.876) 22 bilinear 224 mnasnet_b1 74.658 (25.342) 92.114 (7.886) 4.38 bicubic 224 spnasnet_100 74.084 (25.916) 91.818 (8.182) 4.42 bilinear 224 skresnet18 73.038 (26.962) 91.168 (8.832) 11.9 bicubic 224 mobilenetv2_100 72.978 (27.022) 91.016 (8.984) 3.5 bicubic 224 resnet18d 72.260 (27.740) 90.696 (9.304) 11.7 bicubic 224 seresnet18 71.742 (28.258) 90.334 (9.666) 11.8 bicubic 224","title":"Self-trained Weights"},{"location":"results/#ported-and-other-weights","text":"For weights ported from other deep learning frameworks (Tensorflow, MXNet GluonCV) or copied from other PyTorch sources, please see the full results tables for ImageNet and various OOD test sets at in the results tables . Model code .py files contain links to original sources of models and weights.","title":"Ported and Other Weights"},{"location":"scripts/","text":"Scripts A train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the PyTorch Imagenet Examples . I have added significant functionality over time, including CUDA specific performance enhancements based on NVIDIA's APEX Examples . Training Script The variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a train and validation folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value: ./distributed_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4 NOTE: It is recommended to use PyTorch 1.7+ w/ PyTorch native AMP and DDP instead of APEX AMP. --amp defaults to native AMP as of timm ver 0.4.3. --apex-amp will force use of APEX components if they are installed. Validation / Inference Scripts Validation and inference scripts are similar in usage. One outputs metrics on a validation set and the other outputs topk class ids in a csv. Specify the folder containing validation images, not the base as in training script. To validate with the model's pretrained weights (if they exist): python validate.py /imagenet/validation/ --model seresnext26_32x4d --pretrained To run inference from a checkpoint: python inference.py /imagenet/validation/ --model mobilenetv3_large_100 --checkpoint ./output/train/model_best.pth.tar","title":"Scripts"},{"location":"scripts/#scripts","text":"A train, validation, inference, and checkpoint cleaning script included in the github root folder. Scripts are not currently packaged in the pip release. The training and validation scripts evolved from early versions of the PyTorch Imagenet Examples . I have added significant functionality over time, including CUDA specific performance enhancements based on NVIDIA's APEX Examples .","title":"Scripts"},{"location":"scripts/#training-script","text":"The variety of training args is large and not all combinations of options (or even options) have been fully tested. For the training dataset folder, specify the folder to the base that contains a train and validation folder. To train an SE-ResNet34 on ImageNet, locally distributed, 4 GPUs, one process per GPU w/ cosine schedule, random-erasing prob of 50% and per-pixel random value: ./distributed_train.sh 4 /data/imagenet --model seresnet34 --sched cosine --epochs 150 --warmup-epochs 5 --lr 0.4 --reprob 0.5 --remode pixel --batch-size 256 --amp -j 4 NOTE: It is recommended to use PyTorch 1.7+ w/ PyTorch native AMP and DDP instead of APEX AMP. --amp defaults to native AMP as of timm ver 0.4.3. --apex-amp will force use of APEX components if they are installed.","title":"Training Script"},{"location":"scripts/#validation-inference-scripts","text":"Validation and inference scripts are similar in usage. One outputs metrics on a validation set and the other outputs topk class ids in a csv. Specify the folder containing validation images, not the base as in training script. To validate with the model's pretrained weights (if they exist): python validate.py /imagenet/validation/ --model seresnext26_32x4d --pretrained To run inference from a checkpoint: python inference.py /imagenet/validation/ --model mobilenetv3_large_100 --checkpoint ./output/train/model_best.pth.tar","title":"Validation / Inference Scripts"},{"location":"training_hparam_examples/","text":"Training Examples EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5 These params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model efficientnet_b2 -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .016 MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5 This params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model mixnet_xl -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .969 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.3 --amp --lr .016 --dist-bn reduce SE-ResNeXt-26-D and SE-ResNeXt-26-T These hparams (or similar) work well for a wide range of ResNet architecture, generally a good idea to increase the epoch # as the model size increases... ie approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti cards: ./distributed_train.sh 2 /imagenet/ --model seresnext26t_32x4d --lr 0.1 --warmup-epochs 5 --epochs 160 --weight-decay 1e-4 --sched cosine --reprob 0.4 --remode pixel -b 112 EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5 The training of this model started with the same command line as EfficientNet-B2 w/ RA above. After almost three weeks of training the process crashed. The results weren't looking amazing so I resumed the training several times with tweaks to a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing looked great. I ended up averaging the best checkpoints from all restarts. The result is mediocre at default res/crop but oddly performs much better with a full image test crop of 1.0. EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5 Michael Klachko achieved these results with the command line for B2 adapted for larger batch size, with the recommended B0 dropout rate of 0.2. ./distributed_train.sh 2 /imagenet/ --model efficientnet_b0 -b 384 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .048 ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5 Trained on two older 1080Ti cards, this took a while. Only slightly, non statistically better ImageNet validation result than my first good AugMix training of 78.99. However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch, etc. Unlike my first AugMix runs, I've enabled SplitBatchNorm, disabled random erasing on the clean split, and cranked up random erasing prob on the 2 augmented paths. ./distributed_train.sh 2 /imagenet -b 64 --model resnet50 --sched cosine --epochs 200 --lr 0.05 --amp --remode pixel --reprob 0.6 --aug-splits 3 --aa rand-m9-mstd0.5-inc1 --resplit --split-bn --jsd --dist-bn reduce EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5 Trained by Andrew Lavin with 8 V100 cards. Model EMA was not used, final checkpoint is the average of 8 best checkpoints during training. ./distributed_train.sh 8 /imagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-connect 0.2 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5 ./distributed_train.sh 2 /imagenet/ --model mobilenetv3_large_100 -b 512 --sched step --epochs 600 --decay-epochs 2.4 --decay-rate .973 --opt rmsproptf --opt-eps .001 -j 7 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 --lr-noise 0.42 0.9 ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5 These params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely 101. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd line below are tuned for 8 GPU training. ./distributed_train.sh 8 /imagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 --weight-decay 1e-4 --sched cosine --reprob 0.4 --recount 3 --remode pixel --aa rand-m7-mstd0.5-inc1 -b 192 -j 6 --amp --dist-bn reduce","title":"Training Examples"},{"location":"training_hparam_examples/#training-examples","text":"","title":"Training Examples"},{"location":"training_hparam_examples/#efficientnet-b2-with-randaugment-804-top-1-951-top-5","text":"These params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model efficientnet_b2 -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .016","title":"EfficientNet-B2 with RandAugment - 80.4 top-1, 95.1 top-5"},{"location":"training_hparam_examples/#mixnet-xl-with-randaugment-805-top-1-949-top-5","text":"This params are for dual Titan RTX cards with NVIDIA Apex installed: ./distributed_train.sh 2 /imagenet/ --model mixnet_xl -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .969 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.3 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.3 --amp --lr .016 --dist-bn reduce","title":"MixNet-XL with RandAugment - 80.5 top-1, 94.9 top-5"},{"location":"training_hparam_examples/#se-resnext-26-d-and-se-resnext-26-t","text":"These hparams (or similar) work well for a wide range of ResNet architecture, generally a good idea to increase the epoch # as the model size increases... ie approx 180-200 for ResNe(X)t50, and 220+ for larger. Increase batch size and LR proportionally for better GPUs or with AMP enabled. These params were for 2 1080Ti cards: ./distributed_train.sh 2 /imagenet/ --model seresnext26t_32x4d --lr 0.1 --warmup-epochs 5 --epochs 160 --weight-decay 1e-4 --sched cosine --reprob 0.4 --remode pixel -b 112","title":"SE-ResNeXt-26-D and SE-ResNeXt-26-T"},{"location":"training_hparam_examples/#efficientnet-b3-with-randaugment-815-top-1-957-top-5","text":"The training of this model started with the same command line as EfficientNet-B2 w/ RA above. After almost three weeks of training the process crashed. The results weren't looking amazing so I resumed the training several times with tweaks to a few params (increase RE prob, decrease rand-aug, increase ema-decay). Nothing looked great. I ended up averaging the best checkpoints from all restarts. The result is mediocre at default res/crop but oddly performs much better with a full image test crop of 1.0.","title":"EfficientNet-B3 with RandAugment - 81.5 top-1, 95.7 top-5"},{"location":"training_hparam_examples/#efficientnet-b0-with-randaugment-777-top-1-953-top-5","text":"Michael Klachko achieved these results with the command line for B2 adapted for larger batch size, with the recommended B0 dropout rate of 0.2. ./distributed_train.sh 2 /imagenet/ --model efficientnet_b0 -b 384 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .048","title":"EfficientNet-B0 with RandAugment - 77.7 top-1, 95.3 top-5"},{"location":"training_hparam_examples/#resnet50-with-jsd-loss-and-randaugment-clean-2x-ra-augs-7904-top-1-9439-top-5","text":"Trained on two older 1080Ti cards, this took a while. Only slightly, non statistically better ImageNet validation result than my first good AugMix training of 78.99. However, these weights are more robust on tests with ImageNetV2, ImageNet-Sketch, etc. Unlike my first AugMix runs, I've enabled SplitBatchNorm, disabled random erasing on the clean split, and cranked up random erasing prob on the 2 augmented paths. ./distributed_train.sh 2 /imagenet -b 64 --model resnet50 --sched cosine --epochs 200 --lr 0.05 --amp --remode pixel --reprob 0.6 --aug-splits 3 --aa rand-m9-mstd0.5-inc1 --resplit --split-bn --jsd --dist-bn reduce","title":"ResNet50 with JSD loss and RandAugment (clean + 2x RA augs) - 79.04 top-1, 94.39 top-5"},{"location":"training_hparam_examples/#efficientnet-es-edgetpu-small-with-randaugment-78066-top-1-93926-top-5","text":"Trained by Andrew Lavin with 8 V100 cards. Model EMA was not used, final checkpoint is the average of 8 best checkpoints during training. ./distributed_train.sh 8 /imagenet --model efficientnet_es -b 128 --sched step --epochs 450 --decay-epochs 2.4 --decay-rate .97 --opt rmsproptf --opt-eps .001 -j 8 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-connect 0.2 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064","title":"EfficientNet-ES (EdgeTPU-Small) with RandAugment - 78.066 top-1, 93.926 top-5"},{"location":"training_hparam_examples/#mobilenetv3-large-100-75766-top-1-92542-top-5","text":"./distributed_train.sh 2 /imagenet/ --model mobilenetv3_large_100 -b 512 --sched step --epochs 600 --decay-epochs 2.4 --decay-rate .973 --opt rmsproptf --opt-eps .001 -j 7 --warmup-lr 1e-6 --weight-decay 1e-5 --drop 0.2 --drop-connect 0.2 --model-ema --model-ema-decay 0.9999 --aa rand-m9-mstd0.5 --remode pixel --reprob 0.2 --amp --lr .064 --lr-noise 0.42 0.9","title":"MobileNetV3-Large-100 - 75.766 top-1, 92,542 top-5"},{"location":"training_hparam_examples/#resnext-50-32x4d-w-randaugment-79762-top-1-9460-top-5","text":"These params will also work well for SE-ResNeXt-50 and SK-ResNeXt-50 and likely 101. I used them for the SK-ResNeXt-50 32x4d that I trained with 2 GPU using a slightly higher LR per effective batch size (lr=0.18, b=192 per GPU). The cmd line below are tuned for 8 GPU training. ./distributed_train.sh 8 /imagenet --model resnext50_32x4d --lr 0.6 --warmup-epochs 5 --epochs 240 --weight-decay 1e-4 --sched cosine --reprob 0.4 --recount 3 --remode pixel --aa rand-m7-mstd0.5-inc1 -b 192 -j 6 --amp --dist-bn reduce","title":"ResNeXt-50 32x4d w/ RandAugment - 79.762 top-1, 94.60 top-5"}]}